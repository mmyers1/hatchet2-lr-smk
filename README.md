# hatchet2-lr-smk
Snakemake pipeline for running HATCHet2 on long-read data

# Setup
## Requirements

* All HATCHet2 requirements, see https://raphael-group.github.io/hatchet/README.html#installation
* my version of HATCHet2: `ont-mm` branch of source code
* bgzip
* samtools
* tabix
* whatshap
* deepTools
* Python 3 with packages: pyBigWig, pandas, click, tqdm, pysam
* snakemake

## Inputs

* Normal BAM file
* Tumor BAM file(s) (1 or more samples)
* * If you have already performed haplotagging, you may supply haplotagged BAM files instead -- these must be placed or softlinked in `{config[data_dir]}/{patient}/{patient}_{sample}_haplotagged.bam`
* Phased VCF file (e.g. via whatshap haplophase)
* Reference genome
* Centromere start and end positions for each chromosome (tables for `GRCh38` and `CHM13v2` are available in `centromere_locations`)
* Cohort CSV table specifying for each patient:  (see `example_cohort.csv` for example)
* * `patient_id`: string identifier
* * `sample_names`: Colon-separated names for each sample 
* * `normal_bam`: Path to long-read BAM file from normal sample
* * `tumor_bams`: Colon-separated path(s) to long-read BAM file(s) (1 or more) from tumor sample(s)
* * `phased_vcf`: Path to phased VCF file (e.g., in Isabl `ONT-WHATSHAP` results)
* YAML config file specifying file paths and input parameters (see `example_config.yaml` for example)


# Running

1. Run the pipeline: `snakemake --snakefile Snakefile --configfile config.yaml`
2. Rerun HATCHet2 as-needed on a per-patient basis, adjusting parameters in the patient's `.ini` file (generated by the pipeline). See `pipeline/run_hatchet.sh` for example script
